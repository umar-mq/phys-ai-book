"use strict";(globalThis.webpackChunkmy_docusaurus_project=globalThis.webpackChunkmy_docusaurus_project||[]).push([[7703],{1142:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>h,default:()=>g,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/multimodal-inputs","title":"4.5 Multimodal Perception: CLIP & SigLIP","description":"Seeing Language","source":"@site/docs/module-4/05-multimodal-inputs.mdx","sourceDirName":"module-4","slug":"/module-4/multimodal-inputs","permalink":"/phys-ai-book/ur/docs/module-4/multimodal-inputs","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"4.4 Structuring Thought: LangChain for Robotics","permalink":"/phys-ai-book/ur/docs/module-4/langchain-for-robotics"},"next":{"title":"4.6 Module 4 Capstone: The Autonomous Humanoid","permalink":"/phys-ai-book/ur/docs/module-4/final-capstone-autonomous-humanoid"}}');var s=a(4848),t=a(8453),r=a(3715),o=a(4021);const l={},h="4.5 Multimodal Perception: CLIP & SigLIP",c={},d=[{value:"Seeing Language",id:"seeing-language",level:2},{value:"Using CLIP for Zero-Shot Detection",id:"using-clip-for-zero-shot-detection",level:3},{value:"Fusion: Audio + Vision",id:"fusion-audio--vision",level:3},{value:"Embedding Spaces &amp; SigLIP",id:"embedding-spaces--siglip",level:2},{value:"Integrating with ROS 2 (The Node Structure)",id:"integrating-with-ros-2-the-node-structure",level:3},{value:"Multimodal Perception: Dekhna aur Samajhna",id:"multimodal-perception-dekhna-aur-samajhna",level:2},{value:"Misaal: Charging Dock Dhoondna",id:"misaal-charging-dock-dhoondna",level:3},{value:"Fusion: Awaaz aur Nazar",id:"fusion-awaaz-aur-nazar",level:3},{value:"Embedding Spaces aur SigLIP",id:"embedding-spaces-aur-siglip",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:3}];function u(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"45-multimodal-perception-clip--siglip",children:"4.5 Multimodal Perception: CLIP & SigLIP"})}),"\n",(0,s.jsxs)(r.A,{level:"novice",language:"english",children:[(0,s.jsx)(n.h2,{id:"seeing-language",children:"Seeing Language"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Problem with Standard Computer Vision"}),'\nTraditional AI (like YOLO) is limited. You train it on "Cats" and "Dogs". If you show it a "Zebra", it gets confused. It has a ',(0,s.jsx)(n.strong,{children:"Fixed Vocabulary"}),'.\nBut humans (and robots) encounter new things every day. If I say "Find the shiny red toy that looks like a dragon", a normal robot fails because it doesn\'t have a "Shiny Red Dragon Toy" class in its database.']}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Solution: Multimodal AI (CLIP)"}),"\nOpenAI created ",(0,s.jsx)(n.strong,{children:"CLIP"})," (Contrastive Language-Image Pre-training).\nIt connects ",(0,s.jsx)(n.strong,{children:"Text"})," and ",(0,s.jsx)(n.strong,{children:"Images"})," in the same mathematical space."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'The math representation (vector) for a picture of a dog is very close to the math representation for the word "Dog".'}),"\n",(0,s.jsx)(n.li,{children:'Crucially, it is also close to "A furry animal" or "Man\'s best friend".'}),"\n"]}),(0,s.jsxs)(n.p,{children:["This allows ",(0,s.jsx)(n.strong,{children:"Open-Vocabulary Detection"}),". You don't need to retrain the robot. You just type what you are looking for."]}),(0,s.jsx)(n.h3,{id:"using-clip-for-zero-shot-detection",children:"Using CLIP for Zero-Shot Detection"}),(0,s.jsxs)(n.p,{children:["We will use a library called ",(0,s.jsx)(n.code,{children:"owl-vit"})," or ",(0,s.jsx)(n.code,{children:"NanoSAM"})," (Segment Anything) powered by CLIP embeddings on the Jetson."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scenario"}),':\nYou want the robot to find his charging dock. You haven\'t trained it on "Charging Dock".']}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Text"}),': "A white plastic charging station with a green light."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Image"}),": The robot's camera feed."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Process"}),": The AI compares the text to every part of the image."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Result"}),": It draws a box around the dock because it matches the description best."]}),"\n"]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Python Implementation (Concept)"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from transformers import Owlv2Processor, Owlv2ForObjectDetection\nfrom PIL import Image\n\n# Load model (Runs on CPU/GPU)\nprocessor = Owlv2Processor.from_pretrained("google/owlv2-base-patch16-ensemble")\nmodel = Owlv2ForObjectDetection.from_pretrained("google/owlv2-base-patch16-ensemble")\n\ndef find_object(image, text_description):\n    inputs = processor(text=text_description, images=image, return_tensors="pt")\n    outputs = model(**inputs)\n    \n    # The model returns bounding boxes for anything matching the text\n    target_sizes = torch.tensor([image.size[::-1]])\n    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.1)\n    \n    return results[0]["boxes"] # Locations of the object\n'})}),(0,s.jsx)(n.h3,{id:"fusion-audio--vision",children:"Fusion: Audio + Vision"}),(0,s.jsx)(n.p,{children:"What if we combine Whisper + CLIP?"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User (Voice)"}),': "Pick up the... uh... blue thingy."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper"}),': Transcribes text "blue thingy".']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CLIP"}),": Looks at the table. Sees a Red Cup, Blue Pen, and Green Apple."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Matching"}),': "Blue thingy" matches "Blue Pen" best.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Robot picks up the pen.\nThis is ",(0,s.jsx)(n.strong,{children:"Multimodal Fusion"}),". The robot uses context to understand vague commands."]}),"\n"]})]}),"\n",(0,s.jsxs)(r.A,{level:"expert",language:"english",children:[(0,s.jsx)(n.h2,{id:"embedding-spaces--siglip",children:"Embedding Spaces & SigLIP"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Contrastive Learning Dynamics"})}),(0,s.jsx)(n.p,{children:"The core of VLA (Vision-Language-Action) is the alignment of feature spaces."}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CLIP (OpenAI)"}),": Uses a contrastive loss to maximize the cosine similarity between correct image-text pairs and minimize incorrect ones."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SigLIP (Google)"}),": Sigmoid Loss for Language Image Pre-training. It decouples the normalization term (Softmax) used in CLIP, allowing for scaling to much larger batch sizes and providing better performance on edge devices."]}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Zero-Shot vs. Few-Shot"}),"\nWhile Zero-Shot (CLIP) is powerful, it lacks spatial precision for grasping.\nFor the Capstone, we use ",(0,s.jsx)(n.strong,{children:"NanoLLaVA"})," or ",(0,s.jsx)(n.strong,{children:"BakLLaVA"})," (on Jetson). These are ",(0,s.jsx)(n.strong,{children:"Visual Instruction Tuning"})," models."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Architecture"}),": A Visual Encoder (SigLIP/CLIP) + Projection Layer + LLM (Llama-3)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Input"}),": Image tokens + Text tokens."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Output"}),": Text (which can be parsed into actions)."]}),"\n"]}),(0,s.jsx)(n.h3,{id:"integrating-with-ros-2-the-node-structure",children:"Integrating with ROS 2 (The Node Structure)"}),(0,s.jsx)(n.p,{children:"Running these models inside a ROS node requires careful threading to avoid blocking the DDS callbacks."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultimodalAgent(Node):\n    def __init__(self):\n        super().__init__('vla_agent')\n        # Subscribers\n        self.sub_img = self.create_subscription(Image, '/camera/rgb', self.img_cb, 1)\n        self.sub_txt = self.create_subscription(String, '/voice_command', self.txt_cb, 10)\n        \n        # State\n        self.latest_image = None\n        self.embedding_db = {} # Database of known object embeddings\n\n    def process_request(self, text_command):\n        # 1. Encode Text\n        text_emb = self.encoder.encode_text(text_command)\n        \n        # 2. Encode Image Patches (Sliding Window)\n        img_patches = extract_patches(self.latest_image)\n        img_embs = self.encoder.encode_images(img_patches)\n        \n        # 3. Vector Similarity Search\n        # Find patch with highest dot product similarity to text\n        scores = torch.matmul(img_embs, text_emb.T)\n        best_patch_idx = torch.argmax(scores)\n        \n        # 4. Project 2D patch to 3D world (Deprojection)\n        # Using Depth Camera info\n        x, y, z = self.deproject(best_patch_idx)\n        \n        # 5. Send Action\n        self.send_goal(x, y, z)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Latency Optimization (TensorRT)"}),'\nSigLIP/CLIP encoders must be exported to TensorRT.\nOn Jetson Orin Nano, standard PyTorch inference for CLIP ViT-L/14 takes ~400ms. TensorRT reduces this to ~60ms, enabling roughly 15Hz perception loops, which is sufficient for "Look-and-Move" behaviors but too slow for visual servoing. For visual servoing, track the object using a lightweight tracker (KCF/CSRT) initialized by CLIP.']})]}),"\n",(0,s.jsxs)(r.A,{level:"novice",language:"urdu",children:[(0,s.jsx)(n.h2,{id:"multimodal-perception-dekhna-aur-samajhna",children:"Multimodal Perception: Dekhna aur Samajhna"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Aam Computer Vision Ka Masla"}),'\nPurani AI (Jaise YOLO) sirf wo cheezain pehchanti hai jo usay sikhayi gayi hon. Agar aap ne usay "Billi" sikhaya hai, to wo "Zebra" ko nahi pehchane gi.\nLekin asli dunya mein hum nayi cheezain dekhte hain. Agar main kahoon "Wo laal rang ka chamakdar khilona dhoondo", to aam robot fail ho jaye ga.']}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hal: CLIP (Multimodal AI)"}),"\nOpenAI ne ",(0,s.jsx)(n.strong,{children:"CLIP"})," banaya hai.\nYeh ",(0,s.jsx)(n.strong,{children:"Text"})," (Lafz) aur ",(0,s.jsx)(n.strong,{children:"Image"})," (Tasveer) ko ek sath jodta hai."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Computer ke liye "Kutta" lafz aur "Kuttay ki tasveer" ka math code (vector) taqreeban aik jaisa hota hai.'}),"\n",(0,s.jsx)(n.li,{children:'Isi tarah "Wafadaar janwar" ka code bhi waisa hi hota hai.'}),"\n"]}),(0,s.jsxs)(n.p,{children:["Is ka faida ye hai ke aap robot ko bina training diye nayi cheezain dhoondne ka keh sakte hain (",(0,s.jsx)(n.strong,{children:"Zero-Shot Detection"}),")."]}),(0,s.jsx)(n.h3,{id:"misaal-charging-dock-dhoondna",children:"Misaal: Charging Dock Dhoondna"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Aap ne bola"}),': "Safed rang ka charger jahan hari light jal rahi hai."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robot ne dekha"}),": Camera feed."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AI ka kaam"}),": CLIP ne aap ke jumlay ko tasveer ke har hissay se match kiya."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Nateeja"}),": Usay wo charger mil gaya, halankay us ne pehle kabhi charger nahi dekha tha."]}),"\n"]}),(0,s.jsx)(n.h3,{id:"fusion-awaaz-aur-nazar",children:"Fusion: Awaaz aur Nazar"}),(0,s.jsx)(n.p,{children:"Agar hum Whisper aur CLIP ko mila dein:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User"}),': "Wo... neeli wali cheez uthao."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper"}),': Text banaya "neeli wali cheez".']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CLIP"}),": Table par dekha. Wahan Laal Cup, Neela Pen, aur Hara Seb tha."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Faisla"}),': "Neeli cheez" sab se zyada "Neela Pen" se match hoti hai.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Robot ne pen utha liya."]}),"\n"]})]}),"\n",(0,s.jsxs)(r.A,{level:"expert",language:"urdu",children:[(0,s.jsx)(n.h2,{id:"embedding-spaces-aur-siglip",children:"Embedding Spaces aur SigLIP"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Lafzon aur Tasveeron ka Milap"})}),(0,s.jsx)(n.p,{children:'VLA (Vision-Language-Action) ki bunyad ye hai ke Lafz aur Tasveer ek hi "Space" mein hon.'}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CLIP"}),": Ye Contrastive Learning use karta hai. Ye koshish karta hai ke Sahi Tasveer aur Sahi Text ka faasla kam ho, aur Ghalat ka zyada ho."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SigLIP"}),": Google ka naya model hai. Ye CLIP se behtar aur tez hai, khaas taur par Jetson jaisay chotay computers par."]}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"NanoLLaVA (Visual Instruction Tuning)"}),"\nCapstone project ke liye hum ",(0,s.jsx)(n.strong,{children:"NanoLLaVA"})," use karenge.\nYe ek chota model hai jo text aur tasveer dono samajhta hai."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Input"}),': Camera ki tasveer + User ka sawal ("Kya main road paar kar sakta hoon?").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Output"}),': Jawab ("Nahi, samne se gaari aa rahi hai").']}),"\n"]}),(0,s.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),(0,s.jsx)(n.p,{children:"Jab hum in models ko ROS node mein chalate hain, to humein speed ka khayal rakhna parta hai."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultimodalAgent(Node):\n    def process_request(self, text_command):\n        # 1. Text ko number (embedding) mein badlo\n        text_emb = self.encoder.encode_text(text_command)\n        \n        # 2. Tasveer ke tukray (patches) ko number mein badlo\n        img_embs = self.encoder.encode_images(current_image)\n        \n        # 3. Match karo (Dot Product)\n        # Jis tukray ka number text ke number se sab se zyada milta hai, wo hamara object hai.\n        best_match = find_similarity(img_embs, text_emb)\n        \n        # 4. 2D pixel ko 3D dunya mein convert karo (Depth camera se)\n        x, y, z = deproject(best_match)\n        \n        # 5. Robot ko bhejo\n        self.send_goal(x, y, z)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Latency Optimization"}),"\nJetson Orin Nano par CLIP chalana bhaari kaam hai (400ms).\nHum ",(0,s.jsx)(n.strong,{children:"TensorRT"})," use kar ke isay 60ms tak laa sakte hain.\nLekin phir bhi, agar robot taizi se hil raha hai, to 60ms bhi late hai. Is liye hum ",(0,s.jsx)(n.strong,{children:"Visual Servoing"})," ke liye CLIP use nahi karte, balkay ek halka tracker (KCF) use karte hain jo CLIP se initialize hota hai."]})]}),"\n",(0,s.jsx)(o.A,{questions:[{question:"What is the key ability of CLIP (Contrastive Language-Image Pre-training)?",options:["It can generate realistic videos","It connects text descriptions to images, allowing Zero-Shot detection","It drives the robot motors directly","It converts audio to text"],correctAnswer:1},{question:"In a multimodal robot pipeline, what is 'Deprojection'?",options:["Projecting the robot's mood onto the user","Converting a 2D pixel coordinate (from an image) into a 3D real-world coordinate (x, y, z) using depth data","Turning off the projector","Deleting the project"],correctAnswer:1},{question:"Urdu: Agar robot ko 'Neela Pen' dhoondna hai, to CLIP model kya karta hai?",options:["Internet par search karta hai","Lafz 'Neela Pen' aur camera ki tasveer ka muwazna (compare) karta hai","Pen banane wali factory ko call karta hai","Sirf neela rang dhoondta hai"],correctAnswer:1}]})]})}function g(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}}}]);